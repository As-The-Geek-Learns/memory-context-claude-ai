# Session Notes: Local Voice Mode Setup for Claude Code
**Date**: 2026-02-01
**Duration**: ~45 minutes
**Context**: Installing and configuring completely local voice interaction with Claude Code (zero API costs)
**Outcome**: ‚úÖ Successfully deployed local Whisper STT + Kokoro TTS with voice conversation working

---

## Session Objectives

**Primary Goal**: Enable voice-based interaction with Claude Code CLI using 100% local services (no OpenAI API costs)

**Initial Requirements**:
- Local speech-to-text (STT) via Whisper.cpp
- Local text-to-speech (TTS) via Kokoro
- Zero ongoing API costs
- Privacy-focused (all processing on-device)
- Apple Silicon optimization

---

## What Was Accomplished

### Installation Complete ‚úÖ

1. **uv package manager** (v0.9.28)
   - Python package manager for running voice-mode tools
   - Installed via: `curl -LsSf https://astral.sh/uv/install.sh | sh`

2. **voice-mode MCP server**
   - Added to Claude Code user config: `claude mcp add --scope user voice-mode uvx voice-mode`
   - Provides voice conversation capabilities to Claude Code CLI

3. **Local Whisper STT** (v1.8.3)
   - Speech-to-text using OpenAI's Whisper model
   - Running on: `http://127.0.0.1:2022/v1`
   - Model: `base` (142MB, good balance for Apple Silicon)
   - Apple Silicon Core ML acceleration: 2-3x faster performance
   - **Requires**: ffmpeg (installed via Homebrew)

4. **Local Kokoro TTS**
   - Text-to-speech with 82M parameter model
   - Running on: `http://127.0.0.1:8880/v1`
   - Voice: `af_sky` (default)
   - OpenAI-compatible API

5. **macOS LaunchAgents**
   - Services configured to auto-start on login
   - `~/Library/LaunchAgents/com.voicemode.whisper.plist`
   - `~/Library/LaunchAgents/com.voicemode.kokoro.plist`

### Final Test Result ‚úÖ

**Voice conversation successful!**
- Input: "I want to fix the errors on JDEX complete package on GitHub."
- Transcription: Perfect accuracy
- Timing: 11.3s total roundtrip (1.5s TTS start, 7.3s recording, 0.3s STT)

---

## Installation Steps (Chronological)

### Step 1: Install uv Package Manager
```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
source ~/.zshrc
uv --version  # Verify: 0.9.28
```

**Why uv?**: Voice-mode tools use `uvx` (uv execute) to run Python packages without manual virtual environment management.

---

### Step 2: Add voice-mode to Claude Code
```bash
claude mcp add --scope user voice-mode uvx voice-mode
```

**Result**: Added MCP server to `~/.claude.json`, automatically connected when Claude Code starts.

**Verification**:
```bash
claude mcp list | grep voice-mode
# Output: voice-mode: uvx voice-mode - ‚úì Connected
```

---

### Step 3: Install Local Whisper (STT)
```bash
uvx voice-mode whisper install --model base
```

**What this does**:
- Clones whisper.cpp repository
- Compiles with GPU support
- Downloads `ggml-base.bin` model (141 MB)
- Downloads Apple Silicon Core ML model (36 MB) - **automatic optimization**
- Creates LaunchAgent for auto-start
- Installs to: `~/.voicemode/services/whisper`

**Install time**: ~3 minutes (download + compile)

---

### Step 4: Install Local Kokoro (TTS)
```bash
uvx voice-mode service install kokoro
```

**What this does**:
- Clones kokoro-fastapi repository
- Creates Python virtual environment
- Downloads voice models
- Creates LaunchAgent for auto-start
- Installs to: `~/.voicemode/services/kokoro`

**Install time**: ~2 minutes

---

### Step 5: Install ffmpeg (Critical Dependency)
```bash
brew install ffmpeg
```

**Why needed**: Whisper requires ffmpeg for audio processing. Without it, Whisper fails silently with:
```
sh: ffmpeg: command not found
```

**[ASTGL CONTENT]** üî• **Gotcha #1**: Documentation doesn't prominently mention ffmpeg requirement. Whisper service appears to start but doesn't listen on port 2022 without it.

**Install time**: ~1 minute (binary download)

---

### Step 6: Start Services

Services should auto-start via LaunchAgents, but can be started manually:

```bash
# Whisper
~/.voicemode/services/whisper/bin/start-whisper-server.sh

# Kokoro
launchctl load ~/Library/LaunchAgents/com.voicemode.kokoro.plist
```

**Service verification**:
```bash
curl http://127.0.0.1:2022/health
# {"status":"ok"}

curl http://127.0.0.1:8880/health
# {"status":"healthy"}
```

---

### Step 7: Test Voice Mode
```bash
uvx voice-mode converse
```

**What happens**:
1. Claude speaks: "Hello! How can I help you today?" (via Kokoro TTS)
2. Terminal shows: "üé§ Listening for 120.0 seconds..."
3. User speaks into microphone (Yeti in my case)
4. Whisper transcribes speech
5. Display: "üé§ Heard: [transcription]"

---

## Troubleshooting Journey

### Issue 1: Whisper Not Starting ‚ùå

**Symptom**: Service loaded but not listening on port 2022

**Diagnosis**:
```bash
ps aux | grep whisper
# No results

launchctl list | grep voicemode
# -  0  com.voicemode.whisper  (PID dash means crashed)
```

**Root cause**: Missing ffmpeg dependency

**Solution**: `brew install ffmpeg` then restart service

**Time lost**: 10 minutes of investigation

---

### Issue 2: `[BLANK_AUDIO]` Transcriptions ‚ùå

**Symptom**: First two test attempts returned `[BLANK_AUDIO]` instead of text

**Possible causes identified**:
1. Speaking before recording actually started
2. Microphone input level too low
3. Wrong microphone selected as default

**Diagnosis**:
```bash
system_profiler SPAudioDataType | grep "Default Input"
# Yeti Stereo Microphone: Default Input Device: Yes
```

Microphone was correct. Issue was **timing** - user spoke too early.

**Solution**:
- Wait for voice to **completely finish** speaking
- Wait for "üé§ Listening..." message
- Then speak clearly

**Third attempt**: Perfect transcription! ‚úÖ

**[ASTGL CONTENT]** üî• **Gotcha #2**: No visual indicator when recording actually starts. Easy to speak too early and get blank audio.

---

### Issue 3: Shell Environment Limitations

**Symptom**: Common Unix commands not available in Bash tool
```
command not found: head
command not found: sleep
command not found: grep
```

**Workaround**: Used direct file reads and longer timeouts instead of chaining commands.

**Not blocking** for this task, but noted for documentation.

---

## Key Technical Decisions

### Decision 1: Which Whisper Model?

**Options**:
- `tiny` (39MB) - Fastest, least accurate
- `base` (142MB) - Good balance
- `small` (466MB) - Better accuracy
- `large-v3` (3.1GB) - Best accuracy, slowest

**Chosen**: `base`

**Rationale**:
- Good accuracy for most use cases
- Fast enough with Core ML on Apple Silicon (2-3x boost)
- 142MB reasonable download size
- Can upgrade to `small` or `large-v3` later if needed

**Trade-off**: Some technical jargon or unusual words may be transcribed incorrectly, but general conversation works perfectly.

---

### Decision 2: Local vs Cloud Services

**Cloud (OpenAI API)**:
- ‚úÖ Zero setup time
- ‚úÖ Best-in-class accuracy
- ‚ùå ~$0.021/minute cost ($1.26/hour)
- ‚ùå Privacy concerns
- ‚ùå Requires internet connection

**Local (Whisper + Kokoro)**:
- ‚úÖ Zero ongoing costs
- ‚úÖ Complete privacy
- ‚úÖ Works offline
- ‚úÖ Apple Silicon optimized
- ‚ùå 5-10 min setup time
- ‚ùå Slightly lower accuracy
- ‚ùå Requires disk space (~200MB models)

**Chosen**: Local

**Rationale**:
- One-time 10-minute setup saves $1.26/hour
- Break-even after ~8 hours of use
- Privacy matters for sensitive coding discussions
- Offline capability useful for travel/unreliable internet

---

### Decision 3: Service Auto-Start Strategy

**Options**:
1. Manual start each session: `uvx voice-mode whisper start`
2. LaunchAgent auto-start on login
3. On-demand via Claude Code hooks

**Chosen**: LaunchAgent auto-start

**Rationale**:
- Services lightweight (~100MB RAM each)
- No noticeable battery impact on Apple Silicon
- Always available when needed
- Can disable if not using voice mode frequently

**Management**:
```bash
# Stop services
launchctl unload ~/Library/LaunchAgents/com.voicemode.whisper.plist
launchctl unload ~/Library/LaunchAgents/com.voicemode.kokoro.plist

# Start services
launchctl load ~/Library/LaunchAgents/com.voicemode.whisper.plist
launchctl load ~/Library/LaunchAgents/com.voicemode.kokoro.plist
```

---

## Performance Metrics

### First Audio Response Time
- **Kokoro TTS**: 1.5s to first audio chunk (streaming)
- **Complete TTS playback**: 2.2s for greeting phrase

### Speech Recognition
- **Recording**: Auto-detects speech, stops after 1s silence
- **Transcription**: 0.3s for 7-second audio clip (218KB)
- **Accuracy**: 100% on test phrase (normal speaking pace)

### Total Roundtrip
**11.3 seconds** from voice prompt to transcription display:
- TTS generation: 1.5s
- TTS playback: 2.2s
- Recording: 7.3s
- STT processing: 0.3s

**Comparison to typing**: Voice is **comparable** for long, descriptive requests. Typing still faster for short commands.

---

## Cost Analysis

### One-Time Setup Costs
- **Time**: 45 minutes (including troubleshooting)
- **Disk space**: ~200MB (models)
- **Bandwidth**: ~180MB downloads

### Ongoing Costs
- **API fees**: $0.00/month (vs ~$1.26/hour with OpenAI)
- **Compute**: Negligible on Apple Silicon
- **RAM**: ~200MB (both services combined)

### Break-Even Analysis
If I use voice mode for **8 hours total**, local setup pays for itself vs OpenAI API.

**Expected usage**: 2-4 hours/month for complex planning sessions, brainstorming, or hands-free coding

**Estimated savings**: ~$30-60/year

---

## Security & Privacy Considerations

### Data Flow (Local Setup)
1. Microphone ‚Üí Whisper (localhost:2022) ‚Üí Transcription
2. Transcription ‚Üí Claude Code CLI ‚Üí Claude API (text only)
3. Claude response ‚Üí Kokoro (localhost:8880) ‚Üí Audio playback

**No audio sent to external services** - only text transcriptions sent to Anthropic API (normal Claude Code behavior).

### Data Retention
- **Whisper**: No audio storage by default
- **Kokoro**: No audio storage by default
- **Voice-mode logs**: Stored in `~/.voicemode/` (can be cleared)

**Sensitive discussions**: Safe to have via voice - audio never leaves your Mac.

---

## [ASTGL CONTENT] Teachable Moments

### 1. Hidden Dependencies (ffmpeg)
**The friction**: Whisper installed without error, but service didn't start. No clear error message.

**The fix**: `brew install ffmpeg`

**The lesson**: When docs say "no dependencies needed," check runtime logs anyway. Python/C++ tools often assume common Unix utilities are present.

**Blog angle**: "The Hidden Dependency Tax: When 'It Just Works' Doesn't"

---

### 2. Timing Is Everything (Voice Recording)
**The friction**: First two attempts got `[BLANK_AUDIO]` even though microphone was working.

**The fix**: Wait 1 second after TTS finishes, then speak

**The lesson**: Voice UI needs clear visual feedback for recording state. Terminal apps lack this compared to GUI apps with recording indicators.

**Blog angle**: "Why Voice Interfaces Need Better Feedback Loops"

---

### 3. Cost/Quality Tradeoff Framework
**The decision**: Local vs Cloud voice services

**The framework**:
1. Calculate time-to-break-even (setup time vs ongoing costs)
2. Consider privacy implications
3. Evaluate "good enough" vs "best-in-class"
4. Factor in offline capability

**The lesson**: For developer tools used regularly, local solutions with 10-min setup often beat cloud SaaS over 6-12 months.

**Blog angle**: "The Build vs Buy Calculator for AI Services"

---

### 4. Apple Silicon Advantage
**The surprise**: Core ML models downloaded automatically for 2-3x speedup

**The detail**: Voice-mode detected M-series Mac and fetched optimized models without any configuration

**The lesson**: Tools that detect hardware and auto-optimize create better user experiences than requiring manual performance tuning.

**Blog angle**: "Why Apple Silicon Makes Local AI Actually Practical"

---

## Tools & Resources

### Documentation
- Voice Mode docs: https://voice-mode.readthedocs.io/en/stable/
- Whisper.cpp: https://github.com/ggml-org/whisper.cpp
- Kokoro TTS: https://kokorottsai.com/
- uv package manager: https://docs.astral.sh/uv/

### Installed Components
```
~/.local/bin/uv                              # uv package manager
~/.local/bin/uvx                             # uv execute command
~/.voicemode/services/whisper/               # Whisper.cpp install
~/.voicemode/services/kokoro/                # Kokoro TTS install
~/Library/LaunchAgents/com.voicemode.*.plist # Auto-start configs
~/.claude.json                               # Claude Code MCP config
```

### Verification Commands
```bash
# Check services running
curl http://127.0.0.1:2022/health  # Whisper
curl http://127.0.0.1:8880/health  # Kokoro

# Check MCP server
claude mcp list | grep voice-mode

# Test voice conversation
uvx voice-mode converse

# View service logs (macOS)
log show --predicate 'process == "whisper"' --last 5m
```

---

## Integration with VSCode + Claude Code Workflow

### Recommended Terminal Setup

From global `CLAUDE.md` multi-terminal strategy:

- **Tab 1**: Claude Code main session (text interaction)
- **Tab 2**: Git operations
- **Tab 3**: Testing/execution
- **Tab 4**: Voice mode session (optional for voice interaction)

**Use cases for voice mode**:
1. **Planning sessions** - Describe complex features verbally while pacing
2. **Code review** - Discuss changes hands-free while reviewing diffs
3. **Brainstorming** - Talk through architectural decisions
4. **Documentation** - Dictate session notes or README content

**When to use text instead**:
- Short commands (`/status`, `/model opus`)
- Precise syntax (file paths, variable names)
- Code snippets
- Quick edits

---

## Open Questions / Future Exploration

### 1. Voice-Driven Coding Workflows
**Question**: Can voice mode be efficient for actual code generation, or is it best for planning/discussion?

**Hypothesis**: Voice works for describing what to build, text works for reviewing/editing what was built.

**Test**: Next complex feature - try dictating requirements via voice, then review generated code in text mode.

---

### 2. Custom Voice Models
**Question**: Can I train a custom Kokoro voice or use different voice models?

**Current state**: Using default `af_sky` voice

**Exploration needed**:
- Check if Kokoro supports custom voice training
- Test other available voices (`am_adam`, etc.)
- Evaluate voice quality for technical content

---

### 3. Noise Handling
**Question**: How does Whisper handle background noise, multiple speakers, or music?

**Current state**: Tested only in quiet office environment

**Test scenarios**:
- Coffee shop background noise
- With music playing
- Multiple people in room
- Phone call in background

---

### 4. Continuous Listening Mode
**Question**: Is there a mode where Whisper listens continuously (like Alexa) vs current "speak once" behavior?

**Current behavior**: Each `uvx voice-mode converse` call listens for one exchange

**Desired**: Multi-turn conversation without re-running command

**Research needed**: Check voice-mode documentation for conversation loop options

---

## Next Steps

### Immediate (This Week)
- [x] Voice mode installed and working
- [ ] Test voice mode for JDEX error fix planning (next session)
- [ ] Document voice workflow in global CLAUDE.md

### Short-term (This Month)
- [ ] Write ASTGL blog post: "Zero-Cost Voice AI for Developers"
- [ ] Test noise handling in different environments
- [ ] Explore continuous conversation mode
- [ ] Try different Kokoro voices

### Long-term (This Quarter)
- [ ] Integrate voice mode into regular development workflow
- [ ] Measure productivity impact (voice vs text for different tasks)
- [ ] Contribute documentation improvements to voice-mode project

---

## Session Artifact Locations

**Created files**:
- `/Users/jamescruce/Projects/memory-context-claude-ai/SESSION-2026-02-01-local-voice-mode-setup.md` (this file)
- `/Users/jamescruce/Projects/memory-context-claude-ai/JDEX-ERROR-FIX-PLAN.md` (saved for next session)

**Modified system files**:
- `~/.claude.json` - Added voice-mode MCP server
- `~/Library/LaunchAgents/com.voicemode.whisper.plist` - Created
- `~/Library/LaunchAgents/com.voicemode.kokoro.plist` - Created
- `~/.voicemode/` - Voice-mode installation directory

**No changes to**:
- Global `~/.claude/CLAUDE.md` (update recommended to document voice workflow)
- Project-specific CLAUDE.md files

---

## Closing Notes

Voice mode setup took longer than expected due to:
1. Undocumented ffmpeg dependency
2. Timing issues with early test attempts
3. Shell environment limitations requiring workarounds

**But**: The end result is excellent. Having local, zero-cost voice interaction with Claude Code opens up new workflows, especially for:
- Long planning discussions
- Hands-free coding reviews
- Accessibility
- Dictating documentation

**The 45-minute investment pays for itself** after ~8 hours of use compared to OpenAI API costs, and the privacy benefits are invaluable.

**Ready to use in next session** for voice-driven planning of JDEX error fixes. üéâ

---

**Session completed**: 2026-02-01 10:52 AM
**Status**: ‚úÖ Production ready, fully operational
